{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise provides an opportunity to apply the concepts and features learned in the previous notebooks.\n",
    "#### Please replace the placeholders (format '<font color='lightgreen'>###something to replace###</font>') including the '<font color='lightgreen'>#</font>' charaters in the pyspark code according to the description .\n",
    "#### The solution can be found in the solution notebook - but don't cheat ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Create a SparkSession*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - let's create a *Spark session* - do you remenber which object you have to import?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession from library\n",
    "from pyspark.sql import ### library for Spark sessions ###\n",
    "spark = SparkSession.builder.appName(\"Exercise\").### function to create or replace a Spark session ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about the Spark session\n",
    "### 'command' to display information about the Spark session ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Load data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need data to work with. For this we load data that is already provided in the local /data/berlin-data folder.<br><br>\n",
    "The data was downloaded from the repository: https://github.com/berlinonline/haeufige-vornamen-berlin.<br>\n",
    "The originator of the data in the original repository folder data/source/ is \"Berlin State Office for Citizens and Regulatory Affairs (LABO)\".<br>\n",
    "The originator of the 'cleaned' data in the original repository folder data/cleaned/ is the \"Berlin State Office for Citizens' and Regulatory Affairs (LABO) / BerlinOnline Stadtportal GmbH & Co. KG\".<br>\n",
    "\n",
    "All data sets contained in the repository are licensed under CC BY 3.0 DE (Creative Commons Attribution 3.0 Germany License)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data content and data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2013, the Berlin city data portal daten.berlin.de has always published lists of the first names of all newborn children and those registered with the registry office at the beginning of the new year. The State Office for Civil and Regulatory Affairs collects the lists from the registry offices in the individual Berlin districts and then publishes them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a folder for each year that contains a CSV file with the frequency of names for each district of Berlin.<br><br>\n",
    "For the years 2012-2016, the column structure of the CSV files is as follows:\n",
    "- 'vorname' specifies the first name\n",
    "- 'anzahl' the total number of children registered with this name\n",
    "- 'geschlecht' the gender of the child\n",
    "\n",
    "From 2017 there is an additional column 'position':\n",
    "- 'position': In the event that a child has been given several first names, position designates the position<br> of the name in the list of names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is nothing to add or exchange in the next code cell!*<br><br> In the next cell, all data in the different subdirectories is read, the information from folder names and file names is added to the datafarm as columns and all data is brought into a uniform schema. Furthermore, all column names and values are converted from German to English and the 'new' dataframe is saved as a file. The file is the basis for the further tasks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lit function from library\n",
    "from pyspark.sql.functions import lit, translate\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "# create schema for dataframe\n",
    "csvSchema = StructType([StructField(\"vorname\", StringType(), True),\n",
    "                             StructField(\"anzahl\", IntegerType(), True),\n",
    "                             StructField(\"geschlecht\", StringType(), True),\n",
    "                             StructField(\"position\", IntegerType(), True),\n",
    "                             StructField(\"year\", IntegerType(), True),\n",
    "                             StructField(\"district\", StringType(), True) \n",
    "                            ])\n",
    "\n",
    "# create empty dataframe\n",
    "df = spark.createDataFrame(spark.sparkContext.emptyRDD(), csvSchema)\n",
    "\n",
    "# 'root' path of berlin data\n",
    "fileDirectory = 'data/berlin-data/cleaned/'\n",
    "# loop over all subdirectories (each subdir = one year of data)\n",
    "for dname in os.listdir(fileDirectory):    \n",
    "    # loop over all files in the 'year' directory\n",
    "    fpath = fileDirectory + dname + \"/\"\n",
    "    for fname in os.listdir(fpath):\n",
    "        df_tmp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\",True)\\\n",
    "            .load(fpath + fname)\n",
    "        # check if schema contains row 'position'. if not add row with default value 1\n",
    "        if not (\"position\" in df_tmp.columns):\n",
    "                    df_tmp = df_tmp.withColumn(\"position\", lit(1))\n",
    "        # get the final component of a pathname. This represents the year.\n",
    "        year = os.path.basename(os.path.dirname(fpath))\n",
    "        # add the year value column\n",
    "        df_tmp = df_tmp.withColumn(\"Year\", lit(year))\n",
    "        # add the disrict value column. The district value is the file name without '.csv' extension.\n",
    "        df_tmp = df_tmp.withColumn(\"District\", lit(fname[:-4]))\n",
    "        # add the df_tmp dataframe to the df dataframe \n",
    "        df = df.union(df_tmp)\n",
    "\n",
    "# rename columns from German -> English\n",
    "df = df.withColumnRenamed(\"vorname\",\"FirstName\") \\\n",
    "    .withColumnRenamed(\"anzahl\",\"NumberOfChildren\")\\\n",
    "    .withColumnRenamed(\"geschlecht\",\"Gender\")\\\n",
    "    .withColumnRenamed(\"position\",\"Position\")\\\n",
    "    .withColumnRenamed(\"year\",\"Year\")\\\n",
    "    .withColumnRenamed(\"district\",\"District\")\n",
    "\n",
    "# change gender value from w (weiblich) -> f (female)    \n",
    "df = df.withColumn('Gender', translate('Gender', 'w', 'f'))\n",
    "# Write DataFrame data to CSV file\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/berlin-data/berlin-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - in the next cells you have to \n",
    "- read a file in csv format and create a dataframe\n",
    "- persist the dataframe with the default storage level (MEMORY_AND_DISK)\n",
    "- print the schema of the dataframe\n",
    "- display the number of rows in the dataframe\n",
    "- display the first ten rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .csv file\n",
    "df = spark.read.format(\"csv\")\\\n",
    "            .### function add an option to the reader ###(\"header\", \"true\")\\\n",
    "            .### function add an option to the reader ###(\"inferSchema\",True)\\\n",
    "            .### function to load data ###(\"data/berlin-data/berlin-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataframe is already cached - if not cache/persist the dataframe\n",
    "if not (df.storageLevel.useMemory) :\n",
    "    df.### function to cache/persist the dataframe ###   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dataframe schema\n",
    "df.### function to display the dataframe schema ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the number of rows in the dataframe\n",
    "df.### function to display the number of rows in the dataframe ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first ten rows of the dataframe\n",
    "df.### function to display the first ten rows of the dataframe###(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Analyse data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how many rows are in the dataframe.<br><br>Exercise - let's find out \n",
    "- how many different first names do exist\n",
    "- how many female and male entries exist\n",
    "- how many entries are made for each year\n",
    "- what is the most popular female first name\n",
    "- what is the most popular male first name\n",
    "- the maximum number of first name in the register (maximum position)\n",
    "- which district has the most entries\n",
    "- How many children have the same name (NumberOfChildren) in average in the year 2021?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different first names do exist?\n",
    "\n",
    "# import ??? function from library\n",
    "from pyspark.sql.functions import ### function to count distinct values ###\n",
    "df.select(### function to count distinct values ###(\"FirstName\")).show()\n",
    "\n",
    "# alternative implementation (return type is integer)\n",
    "df.select(\"FirstName\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different female and male entries exist?\n",
    "df.### function to group the data ###(\"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many entries are made for each year?\n",
    "df.sort(\"Year\").### function to group the data ###(df.Year).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the most popular female first name?\n",
    "df.sort(df.NumberOfChildren.desc()).### function to apply a filter ###(df.Gender == \"f\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the most popular female first name?\n",
    "df.sort(df.NumberOfChildren.desc()).### function to apply a filter ###(df.Gender == \"m\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the maximum number of first names somebody has (maximum position)?\n",
    "\n",
    "# import ??? function from library\n",
    "from pyspark.sql.functions import ### function to get maximum value ###\n",
    "df.select(### function to get maximum value ###(\"Position\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which district has the most entries?\n",
    "\n",
    "# import functions from library\n",
    "from pyspark.sql.functions import ### function to get a column from name ###\n",
    "df.groupBy(\"District\").count().sort(### function to get a column from name ###(\"count\").desc()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many children have the same name (NumberOfChildren) in average in the year 2021?\n",
    "\n",
    "# import functions from library\n",
    "from pyspark.sql.functions import ### function to get average value ###\n",
    "df.filter(df.Year == '2021').### function to compute aggregates ###(### function to get average value ###(\"NumberOfChildren\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Analyse data using 'sql' and 'join' commands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the functions used in above chapter we know want to use 'join' commands.<br><br>\n",
    "Exercise - let's find out \n",
    "- how many first names exist in dictrict mitte but not in district pankow\n",
    "- how many female and male entries exist\n",
    "- how many entries are made for each year\n",
    "- what is the most popular female first name\n",
    "- what is the most popular male first name\n",
    "- the maximum number of first name in the register (maximum position)\n",
    "- which district has the most entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have 'easier' acces to the different data of the districts \n",
    "# we can create new dataframe for the relevant districts\n",
    "\n",
    "df_mitte = df.filter(df.District == 'mitte')\n",
    "df_pankow = df.filter(df.District == 'pankow')\n",
    "df_lichtenberg = df.filter(df.District == 'lichtenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many first names exist in dictrict mitte but not in district pankow? \n",
    "# use dataframe 'join' command\n",
    "\n",
    "df_mitte.join(df_pankow, (df_mitte.FirstName == df_pankow.FirstName), ### add join type in quotes (as string) ###).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many first names exist in dictrict mitte but not in district pankow? \n",
    "# use 'sql' command - hint: don't forget to create local temporary views\n",
    "\n",
    "df_mitte.### function to create or replace a temporary view ###(\"mitte\")\n",
    "df_pankow.### function to create or replace a temporary view ###(\"pankow\")\n",
    "\n",
    "joinDF = spark.### function to use SQL statements ###(\"SELECT count(*) FROM mitte m \\\n",
    "                    LEFT OUTER JOIN pankow p \\\n",
    "                    ON m.FirstName = p.FirstName \\\n",
    "                    WHERE p.FirstName IS NULL \") \\\n",
    "  .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs vs DataFrames and Datasets\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2018/05/rdd-1024x595.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset (RDD)\n",
    "RDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
    "\n",
    "### When to use RDDs?\n",
    "Consider these scenarios or common use cases for using RDDs when:\n",
    "- you want low-level transformation and actions and control on your dataset;\n",
    "- your data is unstructured, such as media streams or streams of text;\n",
    "- you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "- you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and\n",
    "- you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    "\n",
    "### What happens to RDDs in Apache Spark 2.0?\n",
    "You may ask: Are RDDs being relegated as second class citizens? Are they being deprecated?\n",
    "\n",
    "**The answer is a resounding NO!**\n",
    "\n",
    "What’s more, as you will note below, you can seamlessly move between DataFrame or Dataset and RDDs at will—by simple API method calls—and DataFrames and Datasets are built on top of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers.\n",
    "\n",
    "In Spark 2.0, DataFrame APIs will merge with Datasets APIs, unifying data processing capabilities across libraries. Because of this unification, developers now have fewer concepts to learn or remember, and work with a single high-level and type-safe API called `Dataset`.\n",
    "\n",
    "![Spark](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Starting in Spark 2.0, Dataset takes on two distinct APIs characteristics: a strongly-typed API and an untyped API, as shown in the table below. Conceptually, consider DataFrame as an alias for a collection of generic objects Dataset[Row], where a Row is a generic untyped JVM object. Dataset, by contrast, is a collection of strongly-typed JVM objects, dictated by a case class you define in Scala or a class in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed and Un-typed APIs\n",
    "\n",
    "<table class=\"table\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Language</th>\n",
    "<th>Main Abstraction</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Scala</td>\n",
    "<td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Java</td>\n",
    "<td>Dataset[T]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Python*</td>\n",
    "<td>DataFrame</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>R*</td>\n",
    "<td>DataFrame</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> **Note:** *Since Python and R have no compile-time type-safety, we only have untyped APIs, namely DataFrames.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Dataset APIs\n",
    "\n",
    "### 1. Static-typing and runtime type-safety\n",
    "\n",
    "Consider static-typing and runtime safety as a spectrum, with SQL least restrictive to Dataset most restrictive. For instance, in your Spark SQL string queries, you won’t know a syntax error until runtime (which could be costly), whereas in DataFrames and Datasets you can catch errors at compile time (which saves developer-time and costs). That is, if you invoke a function in DataFrame that is not part of the API, the compiler will catch it. However, it won’t detect a non-existing column name until runtime.\n",
    "\n",
    "At the far end of the spectrum is Dataset, most restrictive. Since Dataset APIs are all expressed as lambda functions and JVM typed objects, any mismatch of typed-parameters will be detected at compile time. Also, your analysis error can be detected at compile time too, when using Datasets, hence saving developer-time and costs.\n",
    "\n",
    "All this translates to is a spectrum of type-safety along syntax and analysis error in your Spark code, with Datasets as most restrictive yet productive for a developer.\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/07/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png)\n",
    "\n",
    "### 2. High-level abstraction and custom view into structured and semi-structured data\n",
    "DataFrames as a collection of Datasets[Row] render a structured custom view into your semi-structured data.\n",
    "\n",
    "### 3. Ease-of-use of APIs with structure\n",
    "\n",
    "Although structure may limit control in what your Spark program can do with data, it introduces rich semantics and an easy set of domain specific operations that can be expressed as high-level constructs. Most computations, however, can be accomplished with Dataset’s high-level APIs. For example, it’s much simpler to perform `agg`, `select`, `sum`, `avg`, `map`, `filter`, or `groupBy` operations. \n",
    "\n",
    "### 4. Performance and Optimization\n",
    "Along with all the above benefits, you cannot overlook the space efficiency and performance gains in using DataFrames and Dataset APIs for two reasons.\n",
    "\n",
    "First, because DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. Across R, Java, Scala, or Python DataFrame/Dataset APIs, all relation type queries undergo the same code optimizer, providing the space and speed efficiency. Whereas the Dataset[T] typed API is optimized for data engineering tasks, the untyped Dataset[Row] (an alias of DataFrame) is even faster and suitable for interactive analysis.\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png)\n",
    "\n",
    "Second, since Spark as a compiler understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten’s internal memory representation using Encoders. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds.\n",
    "\n",
    "### When should I use DataFrames or Datasets?\n",
    "- If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "- If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "- If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "- If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "- If you are a R user, use DataFrames.\n",
    "- If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programming language Python is used for the implementation in this course - for this we use 'pyspark. (PySpark documentation https://spark.apache.org/docs/latest/api/python/)\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have defined *Spark configuration* and *Spark context* objects.\n",
    "Now we are using *Spark Session*.\n",
    "\n",
    "SparkSession vs SparkContext<br>\n",
    "Since earlier versions of Spark or Pyspark, SparkContext (JavaSparkContext for Java) is an entry point to Spark programming with RDD and to connect to Spark Cluster, Since Spark 2.0 SparkSession has been introduced and became an entry point to start programming with DataFrame and Dataset.\n",
    "\n",
    "What is SparkContext<br>\n",
    "Spark SparkContext is an entry point to Spark and defined in org.apache.spark package since 1.x and used to programmatically create Spark RDD, accumulators and broadcast variables on the cluster. Since Spark 2.0 most of the functionalities (methods) available in SparkContext are also available in SparkSession. Its object sc is default available in spark-shell and it can be programmatically created using SparkContext class.\n",
    "\n",
    "What is SparkSession<br>\n",
    "SparkSession introduced in version 2.0 is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame and DataSet. It’s object spark is default available in spark-shell and it can be created programmatically using SparkSession builder pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"Structured API\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check (try) if Spark session variable (spark) exists and print information about the Spark context\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print(\"Spark session does not context exist. Please create Spark session first (run cell above).\")\n",
    "else:\n",
    "    configurations = spark.sparkContext.getConf().getAll()\n",
    "    for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 'flight' data from the internet\n",
    "\n",
    "# import Path library\n",
    "from pathlib import Path\n",
    "# check if file already exists\n",
    "if Path('data/flight-data/2015-summary.json').is_file():\n",
    "    print (\"File 2015-summary.json already in data directory - no need to download.\")\n",
    "else:\n",
    "    !wget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json -P data/flight-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/json/2015-summary.json\n",
    "\n",
    "# create a Spark dataframe and read a file in json format\n",
    "df = spark.read.format(\"json\").load(\"data/flight-data/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the schema of a json file (direct from the file content)\n",
    "spark.read.format(\"json\").load(\"data/flight-data/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column.*\n",
    "\n",
    "*If the types in the data (at runtime) do not match the schema, Spark will throw an error. The example that follows shows how to create and enforce a specific schema on a DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import types from library\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a struct object as schema for the json format in the file\n",
    "myManualSchema = StructType([StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "                             StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "                             StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"}) \n",
    "                            ])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema).load(\"data/flight-data/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Types**<br><br>\n",
    "Please see https://spark.apache.org/docs/latest/sql-ref-datatypes.html for Spark version 3.2.1 types list.\n",
    "\n",
    "<table class=\"table\">\n",
    "<tbody><tr>\n",
    "  <th style=\"width:20%\">Data type</th>\n",
    "  <th style=\"width:40%\">Value type in Python</th>\n",
    "  <th>API to access or create a data type</th></tr>\n",
    "<tr>\n",
    "  <td> <b>ByteType</b> </td>\n",
    "  <td>\n",
    "  int or long <br>\n",
    "  <b>Note:</b> Numbers will be converted to 1-byte signed integer numbers at runtime.\n",
    "  Please make sure that numbers are within the range of -128 to 127.\n",
    "  </td>\n",
    "  <td>\n",
    "  ByteType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>ShortType</b> </td>\n",
    "  <td>\n",
    "  int or long <br>\n",
    "  <b>Note:</b> Numbers will be converted to 2-byte signed integer numbers at runtime.\n",
    "  Please make sure that numbers are within the range of -32768 to 32767.\n",
    "  </td>\n",
    "  <td>\n",
    "  ShortType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>IntegerType</b> </td>\n",
    "  <td> int or long </td>\n",
    "  <td>\n",
    "  IntegerType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>LongType</b> </td>\n",
    "  <td>\n",
    "  long <br>\n",
    "  <b>Note:</b> Numbers will be converted to 8-byte signed integer numbers at runtime.\n",
    "  Please make sure that numbers are within the range of\n",
    "  -9223372036854775808 to 9223372036854775807.\n",
    "  Otherwise, please convert data to decimal.Decimal and use DecimalType.\n",
    "  </td>\n",
    "  <td>\n",
    "  LongType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>FloatType</b> </td>\n",
    "  <td>\n",
    "  float <br>\n",
    "  <b>Note:</b> Numbers will be converted to 4-byte single-precision floating\n",
    "  point numbers at runtime.\n",
    "  </td>\n",
    "  <td>\n",
    "  FloatType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>DoubleType</b> </td>\n",
    "  <td> float </td>\n",
    "  <td>\n",
    "  DoubleType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>DecimalType</b> </td>\n",
    "  <td> decimal.Decimal </td>\n",
    "  <td>\n",
    "  DecimalType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>StringType</b> </td>\n",
    "  <td> string </td>\n",
    "  <td>\n",
    "  StringType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>BinaryType</b> </td>\n",
    "  <td> bytearray </td>\n",
    "  <td>\n",
    "  BinaryType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>BooleanType</b> </td>\n",
    "  <td> bool </td>\n",
    "  <td>\n",
    "  BooleanType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>TimestampType</b> </td>\n",
    "  <td> datetime.datetime </td>\n",
    "  <td>\n",
    "  TimestampType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>DateType</b> </td>\n",
    "  <td> datetime.date </td>\n",
    "  <td>\n",
    "  DateType()\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>ArrayType</b> </td>\n",
    "  <td> list, tuple, or array </td>\n",
    "  <td>\n",
    "  ArrayType(<i>elementType</i>, [<i>containsNull</i>])<br>\n",
    "  <b>Note:</b> The default value of <i>containsNull</i> is <i>True</i>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>MapType</b> </td>\n",
    "  <td> dict </td>\n",
    "  <td>\n",
    "  MapType(<i>keyType</i>, <i>valueType</i>, [<i>valueContainsNull</i>])<br>\n",
    "  <b>Note:</b> The default value of <i>valueContainsNull</i> is <i>True</i>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>StructType</b> </td>\n",
    "  <td> list or tuple </td>\n",
    "  <td>\n",
    "  StructType(<i>fields</i>)<br>\n",
    "  <b>Note:</b> <i>fields</i> is a Seq of StructFields. Also, two fields with the same\n",
    "  name are not allowed.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>StructField</b> </td>\n",
    "  <td> The value type in Python of the data type of this field\n",
    "  (For example, Int for a StructField with the data type IntegerType) </td>\n",
    "  <td>\n",
    "  StructField(<i>name</i>, <i>dataType</i>, [<i>nullable</i>])<br>\n",
    "  <b>Note:</b> The default value of <i>nullable</i> is <i>True</i>.\n",
    "  </td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the content of the dataframe\n",
    "df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first row of the dataframe\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a row object in spark DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asDict returns a object from type dict after selecting the first row of the dataframe.\n",
    "# Specifing the name/key of the name value pair (in brackets)returns the value \n",
    "df.first().asDict()['DEST_COUNTRY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispay the first three entries of the dataframe\n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns and Expressions\n",
    "\n",
    "There are a lot of different ways to construct and refer to columns but the two simplest ways are\n",
    "by using the `col` or `column` functions.<br><br>\n",
    "Unfortunately, there is a bug in Spark version 3.1.2 that prevents you from using the column function \n",
    "as intended. I.e. in this specific Spark version we only use the col function. \n",
    "Here is the link to the problem, for info: https://issues.apache.org/jira/browse/SPARK-35643\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from library\n",
    "from pyspark.sql.functions import col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column \"DEST_COUNTRY_NAME\" from the dataframe and display two rows\n",
    "df.select(col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark version 3.1.2, there is a bug for the function 'column'. More about the bug can be read here: https://issues.apache.org/jira/browse/SPARK-35643.<br><br>Therefore, the SPARK version is determined in the following cells when using the function 'Column' and the function is not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (spark.version != \"3.1.2\"):\n",
    "    # select column \"DEST_COUNTRY_NAME\" from the dataframe and display two rows\n",
    "    df.select(column(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import expression from library\n",
    "from pyspark.sql.functions import expr\n",
    "# expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value \n",
    "# as an expression argument to Pyspark built-in functions.\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as Destination\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select and selectExpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column \"DEST_COUNTRY_NAME\" from the dataframe and display two rows\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns \"DEST_COUNTRY_NAME\" and \"ORIGIN_COUNTRY_NAME\" from the dataframe and display two rows\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (spark.version != \"3.1.2\"):\n",
    "    # select column \"DEST_COUNTRY_NAME\" from the dataframe in three different ways and display two rows\n",
    "    df.select(expr(\"DEST_COUNTRY_NAME\"),\n",
    "                    col(\"DEST_COUNTRY_NAME\"),\n",
    "                    column(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column \"DEST_COUNTRY_NAME\" from the dataframe with an expression and display two rows\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column \"DEST_COUNTRY_NAME\" from the dataframe with an expression, set an alias and display two rows\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column \"DEST_COUNTRY_NAME\" from the dataframe with an expression, select the same coloumn by name\n",
    "# and display two rows\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all original columns from the dataframe and add an additional column with a condition\n",
    "df.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and count the distinct values in column \"DEST_COUNTRY_NAME\" and the average of the count column\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column with literal 1\n",
    "# The lit() function is used to add constant or literal value as a new column to the dataframe.\n",
    "\n",
    "# import lit function from library\n",
    "from pyspark.sql.functions import lit\n",
    "# add column \"numberOne\" with literal value 1\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn() is a DataFrame function that is used to add a new column to DataFrame, change the value of \n",
    "# an existing column, convert the datatype of a column or derive a new column from an existing column\n",
    "\n",
    "# add new column \"withinCountry\" using an expression to determine the values\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "# withColumnRenamed() is used to rename one column or multiple DataFrame column names.\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Columns\n",
    "# Spark DataFrame provides a drop() method to drop a column/field from a DataFrame/Dataset. \n",
    "# Thedrop() method also used to remove multiple columns at a time from a Spark DataFrame/Dataset.\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing a Column’s Type (cast)\n",
    "# Add column \"count2\" from column \"count\" with new type \"long\" and display the dataframe schema\n",
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\")).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display only rows with count column value < 2 using the filter function\n",
    "df.filter(col(\"count\") < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display only rows with count column value < 2 using the where function \n",
    "\n",
    "# The where() operator is often used if people have a SQL background. \n",
    "# filter() and where() functions operate exactly the same.\n",
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter row by combining multiple where() functions\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Unique Rows\n",
    "# display the number of unique (distinct) rows from the field \"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\"\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the number of unique (distinct) rows from the field \"ORIGIN_COUNTRY_NAME\"\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark sampling is a mechanism to get random sample records from the dataset\n",
    "\n",
    "# Seed for sampling (default a random seed). Used to reproduce same random sampling\n",
    "seed = 5\n",
    "# Sample with replacement or not (default False).\n",
    "withReplacement = False\n",
    "# By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset.\n",
    "fraction = 0.5\n",
    "# get samples of the dataset and count the number of samples\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating and Appending Rows (Union)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import row functions from library\n",
    "from pyspark.sql import Row\n",
    "# save schema of dataframe df in variable schema\n",
    "schema = df.schema\n",
    "# Row can be used to create a row objects by using named arguments\n",
    "# this command create two new structs for the new rows. Each row with three columns - which fits the schema\n",
    "newRows = [Row(\"New Country\", \"Other Country\", 5),Row(\"New Country 2\", \"Other Country 3\", 1)]\n",
    "# create the new RDD in parallel/concurrent tasks\n",
    "# remember: parallelized collections are created by calling SparkContext’s parallelize method on an existing \n",
    "#           iterable or collection in your driver program. The elements of the collection are copied to form \n",
    "#           a distributed dataset that can be operated on in parallel.\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "# create a new dataframe from the RDD using the same schema as in df\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the new dataframe schema and content\n",
    "newDF.printSchema()\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema.\n",
    "# If schemas are not the same it returns an error.\n",
    "\n",
    "# combine dataframe df with dataframe newDF and select only the rows \n",
    "# with \"count = 1\" and \"ORIGIN_COUNTRY_NAME != 'United States'\"\n",
    "df.union(newDF).where(\"count = 1\").where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark dataframe/dataset class provides sort() function to sort on one or more columns. \n",
    "# By default, it sorts by ascending order.\n",
    "df.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, Spark dataFrame/dataset class also provides orderBy() function to sort on one or more columns. \n",
    "# By default, it also orders by ascending.\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe by two columns\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By default it sorts in ascending order but if you want to explicitly define the order use desc and asc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import desc, asc functions from library\n",
    "from pyspark.sql.functions import desc, asc\n",
    "# sort the dataframe on column 'count' in descending order by using an expression\n",
    "df.orderBy(expr(\"count desc\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe on two columns. One in ascending order and one in descending order.\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The limit clause is used to constrain the number of rows returned by the SELECT statement. \n",
    "# In general, this clause is used in conjunction with ORDER BY to ensure that the results are deterministic.\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartition and Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PySpark you can get the current length/size of partitions by running getNumPartitions() of RDD class, \n",
    "# so to use with DataFrame first you need to convert to RDD.\n",
    "# RDD: rdd.getNumPartitions()\n",
    "# dataframe (convert to RDD first): \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition() is used to increase or decrease the RDD/dataframe\n",
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition dataframe and display number of partitions\n",
    "df.repartition(5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you know that you’re going to be filtering by a certain column often, \n",
    "# it can be worth repartitioning based on that column\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition with defined number of partitions and partition column\n",
    "# If number of partitions is not specified, the default number of partitions is used.\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce() is used to only decrease the number of partitions in an efficient way.\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Types of Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 'retail' data from the internet\n",
    "\n",
    "# import Path library\n",
    "from pathlib import Path\n",
    "# check if file already exists\n",
    "if Path('data/retail-data/by-day/2010-12-01.csv').is_file():\n",
    "    print (\"File 2010-12-01.csv already in data directory - no need to download.\")\n",
    "else:\n",
    "    !wget !wget https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv -P data/retail-data/by-day/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/by-day/2010-12-01.csv\n",
    "\n",
    "# Read a file in csv format. The path can be either a single CSV file or a directory of CSV files\n",
    "# options:\n",
    "#          header: uses the first line as names of columns. \n",
    "#          inferSchema: Infers the input schema automatically from data. It requires one extra pass over the data.\n",
    "# See this page for all options: https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"data/retail-data/by-day/2010-12-01.csv\")\n",
    "# display the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lit function from library\n",
    "from pyspark.sql.functions import lit\n",
    "# 'read' a three columns row. Each with a different data type and the value '5'\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Booleans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import col function from library\n",
    "# col returns a Column based on the given column name.\n",
    "from pyspark.sql.functions import col\n",
    "# select only columns \"InvoiceNo\" and \"Description\" AND filter data where \"InvoiceNo\" != 536365\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import instr function from library\n",
    "from pyspark.sql.functions import instr\n",
    "# create the 'where' condition '(UnitPrice > 600)'\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "# instr locates the position of the first occurrence of substr column in the given string. \n",
    "# Returns null if either of the arguments are null.\n",
    "# create the 'where' condition '(instr(Description, POSTAGE) >= 1)'\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "# isin function is a boolean expression that is evaluated to true if the value of this expression is \n",
    "# contained by the evaluated values of the arguments. \n",
    "# create 'where' condition to check if the value in column StockCode is in the list of values. \n",
    "# The list in this case only contains \"DOT\"\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n",
    "# the output only displays row where\n",
    "#    - column StockCode contains the value \"DOT\" \n",
    "#      AND\n",
    "#    - ('column UnitPrice value is greater than 600' OR 'The position of the word \"POSTAGE\"\n",
    "#       in column Description >= 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same 'where' conditions as in the cell above with different notations\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "# create a true/false column \"isExpensive\" based on the where condition in the second argument\n",
    "# filter all rows by \"isExpensive\" == True\n",
    "# and display on the columns \"unitPrice\" and\"isExpensive\"\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a true/false column \"isExpensive\" based on an expression (Unit Price > 250)\n",
    "# filter all rows by \"isExpensive\" == True\n",
    "# and display on the columns \"Description\" and \"unitPrice\"\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import expr and pow functions from library\n",
    "from pyspark.sql.functions import expr, pow\n",
    "# pow returns the value of the first argument raised to the power of the second argument.\n",
    "# fabricatedQuantity is of type 'column' ()\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "# display the Quantity and UnitPrice of the dataframe\n",
    "df.select(col(\"CustomerId\"), col(\"Quantity\"), col(\"UnitPrice\")).show(2)\n",
    "# display the new column in table with name \"realQuantity\"\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result as in cell above - using selectExpr\n",
    "# the function selectExpr() takes a set of SQL expressions in a string to execute\n",
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lit, round and bround functions from library\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "# lit: creates a column of literal value\n",
    "# round: round the given value to scale decimal places using HALF_UP rounding mode if scale >= 0 or \n",
    "#        at integral part when scale < 0.\n",
    "# bround: round the given value to scale decimal places using HALF_EVEN rounding mode if scale >= 0 or \n",
    "#        at integral part when scale < 0.\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corr function from library\n",
    "from pyspark.sql.functions import corr\n",
    "# corr: calculates the correlation of two columns of a dataframe as a double value.\n",
    "# output type float\n",
    "print( df.stat.corr(\"Quantity\", \"UnitPrice\") )\n",
    "# output type dataframe\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'describe' computes basic statistics for numeric and string columns.\n",
    "# This include count, mean, stddev, min, and max. \n",
    "# If no columns are given, this function computes statistics for all numerical or string columns.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import count, mean, stddev_pop, min and max functions from library\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max\n",
    "# 'approxQuantile' calculates the approximate quantiles of numerical columns of a dataframe\n",
    "# parameters are:\n",
    "#    column name. Can be a single column name, or a list of names for multiple columns.\n",
    "colName = \"UnitPrice\"\n",
    "#    probabilities. A list of quantile probabilities. Each number must belong to [0, 1].\n",
    "quantileProbs = [0.5]\n",
    "#    relativeError. The relative target precision to achieve (>= 0). \n",
    "#                   If set to zero, the exact quantiles are computed, which could be very expensive.\n",
    "relError = 0.05\n",
    "# calculate the approximate quantiles of column \"UnitPrice\" with probability 0,5 and relativeError 0,05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'crosstab' computes a pair-wise frequency table of the given columns. Also known as a contingency table.\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding frequent items for columns, possibly with false positives.\n",
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import monotonically_increasing_id function from library\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# monotonically_increasing_id: A column that generates monotonically increasing 64-bit integers.\n",
    "# The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
    "df.select(monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initcap function from library\n",
    "from pyspark.sql.functions import initcap\n",
    "# initcap: Translate the first letter of each word to upper case in the sentence.\n",
    "df.select(initcap(col(\"Description\"))).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lower and upper function from library\n",
    "from pyspark.sql.functions import lower, upper\n",
    "# lower: Converts a string expression to lower case.\n",
    "# upper: Converts a string expression to upper case.\n",
    "df.select(col(\"Description\"),\n",
    "lower(col(\"Description\")),\n",
    "upper(lower(col(\"Description\")))).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lit, ltrim, rtrim, rpad, lpad and trim functions from library\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "# lit: Creates a Column of literal value.\n",
    "# ltrim: Trim the spaces from left end for the specified string value.\n",
    "# rtrim: Trim the spaces from right end for the specified string value.\n",
    "# rpad: Right-pad the string column to width len with pad.\n",
    "# lpad: Left-pad the string column to width len with pad.\n",
    "# trim: Trim the spaces from both ends for the specified string column.\n",
    "df.select(\n",
    "ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "trim(lit(\"    HELLO   \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions\n",
    "# import regexp_replace function from library\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "# regexp_replace: Replace all substrings of the specified string value that match regexp with rep.\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "# replace the stings BLACK\" or WHITE or RED or GREEN or BLUE with the string COLOR\n",
    "df.select(\n",
    "regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "col(\"Description\")).show(2, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import translate function from library\n",
    "from pyspark.sql.functions import translate\n",
    "# translate(srcCol, matching, replace):\n",
    "# A function translate any character in the srcCol by a character in matching. \n",
    "# The characters in replace is corresponding to the characters in matching. \n",
    "# The translate will happen when any character in the string matching with the character in the matching.\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regexp_extract function from library\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "# regexp_extract: Extract a specific group matched by a regex, from the specified string column. \n",
    "# If the regex did not match, or the specified group did not match, an empty string is returned.\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "col(\"Description\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import instr function from library\n",
    "from pyspark.sql.functions import instr\n",
    "# instr: Locate the position of the first occurrence of substr column in the given string. \n",
    "# Returns null if either of the arguments are null.\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "# select only the decription columns if the text contains the string 'BLACK' or 'WHITE'\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\")\\\n",
    ".select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import current_date and current_timestamp functions from library\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "# current_date: Returns the current date at the start of query evaluation as a DateType column. \n",
    "#               All calls of current_date within the same query return the same value.\n",
    "# current_timestamp:  Returns the current timestamp at the start of query evaluation as a TimestampType column. \n",
    "#               All calls of current_timestamp within the same query return the same value.\n",
    "#\n",
    "# create a dataframe with 10 rows and 3 columns (id, today, now)\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the dataframe\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import date_add and date_sub functions from library\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "# date_add: Returns the date that is 'days' (second parameter) days after start (first parameter)\n",
    "# date_sub: Returns the date that is 'days' (second parameter) days before start (first parameter)\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datediff, months_between and to_date functions from library\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "# datediff: Returns the number of days from start (second parameter)to end (first parameter).\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months_between: \n",
    "# Returns number of months between 'start' date and 'end' date. \n",
    "# If 'start' date is later than 'end' date, then the result is positive. \n",
    "# If 'start' date and 'end' date are on the same day of month, or both are the last day of month, \n",
    "# returns an integer (time of day will be ignored). \n",
    "# The result is rounded off to 8 digits unless roundOff is set to False (optional parameter 'roundOff=True').\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to_date and lit functions from library\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "# to_date: The function is used to format string (StringType) to date (DateType) column.\n",
    "#          It takes the first argument as a date string and the second argument (optional) \n",
    "#          takes the pattern the date is in the first argument.\n",
    "#          Overview datetime patterns: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to_date function from library\n",
    "from pyspark.sql.functions import to_date\n",
    "# to_date: The function is used to format string (StringType) to date (DateType) column.\n",
    "#          It takes the first argument as a date string and the second argument (optional) \n",
    "#          takes the pattern of the date which is in the first argument.\n",
    "#          Overview datetime patterns: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to_timestamp function from library\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "# to_timestamp: The function is used to convert a String to Timestamp.\n",
    "#               The converted time would be in a default format of MM-dd-yyyy HH:mm:ss.SSS.\n",
    "#               When the format is not in this format the function returns null.\n",
    "#               An optional second parameter is an additional String argument to specify the format \n",
    "#               of the input Timestamp - if the date string format is not 'default'.\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the 'working with Nulls' examples we want to investigate the dataframe content.<br>\n",
    "This helps to understand the output of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import when, count and col functions from library\n",
    "from pyspark.sql.functions import when, count, col\n",
    "print(\"Schema of the dataset:\")\n",
    "df.printSchema()\n",
    "print(\"List of all columns with the number of NULL values in each column:\")\n",
    "df.select([count(when (col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import coalesce function from library\n",
    "from pyspark.sql.functions import coalesce\n",
    "# coalesce() is used to only decrease the number of partitions in an efficient way.\n",
    "# If you specify the number of partitions (type integer) as a parameter the function returns a \n",
    "# new DataFrame that has exactly numPartitions partitions.\n",
    "# If the parameters are dataframe colums the function returns the first non-null value among \n",
    "# the given columns or null if all columns are null.\n",
    "# Coalesce requires at least one column and all columns have to be of the same or compatible types.\n",
    "print(\"Rows with value in Description or CustomerID column is NULL:\")\n",
    "df.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\"), col(\"CustomerId\")).\\\n",
    "    filter(\"Description is NULL OR CustomerID is NULL\").show(5, False)\n",
    "\n",
    "print(\"Rows with coalesce(Description, CustomerId) output:\")\n",
    "df.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\"), col(\"CustomerId\"),\\\n",
    "          coalesce(col(\"Description\"), col(\"CustomerId\"))).\\\n",
    "              filter(\"InvoiceNo = 536414 OR InvoiceNo = 536544\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are using the 'na functions'. <dataframe>'.na' returns a DataFrameNaFunctions for handling missing values.\n",
    "<br>Methods within 'na' are:\n",
    "\n",
    "  drop([how, thresh, subset]) - Returns a new DataFrame omitting rows with null values.<br>\n",
    "  fill(value[, subset]) - Replace null values, alias for na.fill().<br>\n",
    "  replace(to_replace[, value, subset]) - Returns a new DataFrame replacing a value with another value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna() returns a new DataFrame omitting rows with null values. \n",
    "# DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.\n",
    "\n",
    "# The function can take 3 optional parameters that are used to remove Rows with NULL values \n",
    "# on single, any, all, multiple DataFrame columns.\n",
    "print(\"number of rows before drop(): \" + str(df.count()))\n",
    "print(\"number of rows after drop(): \" + str(df.na.drop().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first parameter 'how' takes values ‘any’ or ‘all’. \n",
    "# By using ‘any’, drop a row if it contains NULLs on any columns. \n",
    "print(\"number of rows before drop('any'): \" + str(df.count()))\n",
    "print(\"number of rows after drop('any'): \" + str(df.na.drop(\"any\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first parameter 'how' takes values ‘any’ or ‘all’. \n",
    "# By using ‘all’, drop a row only if all columns have NULL values. Default is ‘any’.\n",
    "print(\"number of rows before drop('all'): \" + str(df.count()))\n",
    "print(\"number of rows after drop('all'): \" + str(df.na.drop(\"all\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset can be used to select the columns for NULL values. Default is ‘None.\n",
    "print(\"number of rows before drop('all') with subset: \" + str(df.count()))\n",
    "print(\"number of rows after drop('all'): \" + str(df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() \n",
    "# are aliases of each other.\n",
    "dfNew = df.na.fill(\"filled-null-describtion\",[\"Description\"]).\\\n",
    "           na.fill(0,[\"CustomerId\"])\n",
    "\n",
    "print(\"Some (selected) rows with original value in columns Description and CustomerID:\")\n",
    "df.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\"), col(\"CustomerId\")).\\\n",
    "    filter(\"Description is NULL OR CustomerID is NULL\").show(5, False)\n",
    "\n",
    "print(\"Rows with null values 'filled' output:\")\n",
    "dfNew.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\"), col(\"CustomerId\")).\\\n",
    "              filter(\"InvoiceNo = 536414 OR InvoiceNo = 536544\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.fillna() or DataFrameNaFunctions.fill() is used to replace NULL/None values on all or \n",
    "# selected multiple DataFrame columns with either zero(0), empty string, space, or any constant \n",
    "# literal values.\n",
    "print(\"Rows with value in Description is NULL:\")\n",
    "df.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\")).\\\n",
    "    filter(\"Description is NULL\").show(5, False)\n",
    "    \n",
    "# execute na.fill on dataframe and display only selected output rows    \n",
    "print(\"Rows with NULL values replaced by literal value:\")\n",
    "df.na.fill(\"All Null values become this string\").\\\n",
    "    select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\")).\\\n",
    "              filter(df.InvoiceNo.isin([536414, 536545, 536546, 536547, 536549, 536543])).show(20, False)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace: Returns a new DataFrame replacing a value with another value.\n",
    "# DataFrame.replace() and DataFrameNaFunctions.replace() are aliases of each other. \n",
    "# Values to_replace and value must have the same type and can only be numerics, booleans, or strings. \n",
    "# Value can have None. When replacing, the new value will be cast to the type of the existing column.\n",
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\").\\\n",
    "    filter(\"Description is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import size function from library\n",
    "from pyspark.sql.functions import size, split\n",
    "# size(): returns the length of the array or map stored in the column.\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) # shows 5 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import array_contains function from library\n",
    "from pyspark.sql.functions import array_contains\n",
    "# array_contains: returns null if the array is null, true if the array contains the given value, \n",
    "#                 and false otherwise.\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import split function from library\n",
    "from pyspark.sql.functions import split, explode\n",
    "# explode: Returns a new row for each element in the given array or map.\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MapType (also called map type) is a data type to represent Python Dictionary (dict) to store \n",
    "# key-value pair, a MapType object comprises three fields, keyType (a DataType), valueType (a DataType) \n",
    "# and valueContainsNull (a BooleanType). Last field is optional.\n",
    "\n",
    "# Maps are created by using the map function and key-value pairs of columns\n",
    "\n",
    "# import create_map function from library\n",
    "from pyspark.sql.functions import create_map\n",
    "# create a the map in new column complex_map from columns Description and InvoiceNo\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2, False)\n",
    "# print the schema of the map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to use the map within 'queries'\n",
    "# this selectExpr only selects the value within the map if the key value is \"WHITE METAL LANTERN\". \n",
    "# Returns null if the key value is not found.\n",
    "df.select(col(\"Description\"), create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"Description\",\"complex_map['WHITE METAL LANTERN']\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expode function on a MapType selects the key, value paires of the map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"explode(complex_map)\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark build in capabilities. \n",
    "UDF is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL’s DSL for transforming Datasets.<br><br>\n",
    "UDFs created using the tags @udf can only be used in DataFrame APIs but not in Spark SQL. To use a UDF in Spark SQL, you have to register it using \n",
    "spark.udf.register. Notice that spark.udf.register can not only register UDFs but also a regular Python function (in which case you have to specify \n",
    "return types).<br><br> Note: UDF’s are the most expensive operations hence only should only be used when there is no choice and when essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe - which will be used in this section\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "udfExampleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the UDF 'power3' \n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# ececute the UDF\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import udf function from library\n",
    "from pyspark.sql.functions import udf\n",
    "# pyspark allows to set a variable as an udf (data type function) - this does NOT register the funtion in Spark SQL\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import col function from library\n",
    "from pyspark.sql.functions import col\n",
    "# the udf can be used in 'queries' with colum values\n",
    "udfExampleDF.select(col(\"num\"), power3udf(col(\"num\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IntegerType and DoubleType functions from library\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "# check if udf is already registerd and if not: register the udf to use the UDF in Spark SQL\n",
    "if not (\"power3py\" in [row[0] for row in spark.catalog.listFunctions()]):\n",
    "    spark.udf.register(\"power3py\", power3, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! cell under investigation: the output should not return nul values !!!\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ALL .csv files from a directory and create five partitions\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "# cache: Persists the DataFrame with the default storage level (MEMORY_AND_DISK).\n",
    "# check if dataframe is already cached - if not cache the dataframe\n",
    "if not (df.storageLevel.useMemory) :\n",
    "    df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe content\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import count function from library\n",
    "from pyspark.sql.functions import count\n",
    "# count the number of rows in column \"StockCode\" in this dataframe\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import countDistinct function from library\n",
    "from pyspark.sql.functions import countDistinct\n",
    "# count the number of DISTINCT values in column \"StockCode\" in this dataframe\n",
    "df.select(countDistinct(\"StockCode\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import approx_count_distinct function from library\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "# approx_count_distinct: Approximate distinct count is much faster at approximately counting the \n",
    "# distinct records rather than doing an exact count, which usually needs a lot of shuffles and other \n",
    "# operations. While the approximate count is not 100% accurate, many use cases can perform equally \n",
    "# well even without an exact count.\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 3364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import first and last functions from library\n",
    "from pyspark.sql.functions import first, last\n",
    "# first: The function by default returns the first values it sees. It will return the first non-null value \n",
    "#        it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "# last: The function by default returns the last values it sees. It will return the last non-null value \n",
    "#       it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IntegerType and DoubleType functions from library\n",
    "from pyspark.sql.functions import min, max\n",
    "# min: returns the minimum value of the expression in a group.\n",
    "# max: returns the maximum value of the expression in a group.\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sum function from library\n",
    "from pyspark.sql.functions import sum\n",
    "# sum: returns the sum of all values in the expression.\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function sumDistinct is deprecated since Spark version 3.2.0: Use sum_distinct() instead.\n",
    "# check if Spark version is < 3.2 and use the equivalent version of the function \n",
    "if (int(spark.version[0:1]) < 3):\n",
    "    # import sumDistinct function from library (spark version < 3)\n",
    "    from pyspark.sql.functions import sumDistinct\n",
    "    # returns the sum of distinct values in the expression\n",
    "    df.select(sumDistinct(\"Quantity\")).show()\n",
    "elif (int(spark.version[2:3]) < 2):\n",
    "    # import sumDistinct function from library (spark version < 3.2)\n",
    "    from pyspark.sql.functions import sumDistinct\n",
    "    # returns the sum of distinct values in the expression\n",
    "    df.select(sumDistinct(\"Quantity\")).show()\n",
    "else:\n",
    "    # import sum_distinct function from library (spark version >= 3.2)\n",
    "    from pyspark.sql.functions import sum_distinct\n",
    "    # returns the sum of distinct values in the expression\n",
    "    df.select(sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"Structured API\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sum, count, avg and expr functions from library\n",
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "# count: returns the number of rows in this dataframe.\n",
    "# sum: returns the sum of all values in the expression.\n",
    "# avg: returns the average of the values in a group.\n",
    "# mean: returns the average of the values in a group.\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transactions\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases\", \n",
    "\"total_transactions\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import var_pop, stddev_pop, var_samp and stddev_samp functions from library\n",
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "# var_pop: returns the population variance of the values in a group.\n",
    "# stddev_pop: returns population standard deviation of the expression in a group.\n",
    "# var_samp: returns the unbiased sample variance of the values in a group.\n",
    "# stddev_samp: returns the unbiased sample standard deviation of the expression in a group.\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness is the measure of the asymmetry of an ideally symmetric probability distribution and \n",
    "# is given by the third standardized moment.\n",
    "# Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to \n",
    "# a normal distribution.\n",
    "\n",
    "# import skewness and kurtosis functions from library\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "# skewness: returns the skewness of the values in a group.\n",
    "# kurtosis: returns the kurtosis of the values in a group.\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corr, covar_pop and covar_samp functions from library\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "# corr: Calculates the correlation of two columns of a DataFrame as a double value. \n",
    "# covar_pop: Returns a new Column for the population covariance of the parameters <col1> and <col2>.\n",
    "# covar_samp: Returns a new Column for the sample covariance of the parameters <col1> and <col2>.\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collect_set and collect_list functions from library\n",
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "# collect_set: returns a set of objects with duplicate elements eliminated.\n",
    "# collect_list: eturns a list of objects with duplicates.\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# groupBy: Groups the DataFrame using the specified columns.\n",
    "# group the dataframe on columns \"InvoiceNo\" and \"CustomerId\" and add the number of rows in the groups\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a different way to count the number of rows in the groups is to use tha agg() function\n",
    "# agg: Compute aggregates and returns the result as a DataFrame.\n",
    "\n",
    "# count the number of row in the groups in two different ways (explicit function and expression)\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "count(\"Quantity\").alias(\"quan\"),\n",
    "expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the agg function work on built-in aggregation functions, such as avg, max, min, sum, count, etc.\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three different dataframes\n",
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")])\\\n",
    ".toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe person\n",
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe graduateProgram\n",
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe sparkStatus\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinExprs is of the form: df(\"key\") === df(\"key\").\n",
    "# this expression defines the column \"graduate_program\" as key from dataframe \"person\"\n",
    "# to be join with the colum \"id\" as key from dataframe \"graduateProgram\".\n",
    "# as 'text' the expression would look like 'person(\"graduate_program\") === graduateProgram(\"id\")'\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case the join expression is used on 'non' key columns - so the join result would not\n",
    "# make sense for the values an therefore it is a wrong join expression.\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as SQL statement this would look like:\n",
    "# SELECT person.id, person.name, person.graduate_program, person.spark_status, \n",
    "#        graduateProgram.id, graduateProgram.degree, graduateProgram.department, graduateProgram.school,\n",
    "# FROM person\n",
    "# INNER JOIN graduateProgram ON person.graduate_program=graduateProgram.id\n",
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# same SQL statement with OUTER JOIN\n",
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same SQL statement with LEFT OUTER JOIN\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# same SQL statement with RIGHT OUTER JOIN\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Left Semi join is similar to inner join difference being leftsemi join returns all columns \n",
    "# from the left DataFrame/Dataset and ignores all columns from the right dataset. In other words, \n",
    "# this join returns columns from the only left dataset for the records match in the right dataset on \n",
    "# join expression, records not matched on join expression are ignored from both left and right datasets.\n",
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# union: Return a new dataframe containing union of rows in this and another dataframe\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "(0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Anti join does the exact opposite of the Spark leftsemi join, leftanti join returns only \n",
    "# columns from the left DataFrame/Dataset for non-matched records.\n",
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross join simply combines each row of the first table with each row of the second table.\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SQL queries directly with the dataframe, you will need to register it to a temporary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createOrReplaceTempView: Creates or replaces a local temporary view with this dataframe.\n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this dataframe.\n",
    "\n",
    "# read a json file and create a local temporary view in the current SparkSession\n",
    "spark.read.json(\"data/flight-data/2015-summary.json\")\\\n",
    ".createOrReplaceTempView(\"some_sql_view\") # DF => SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To issue any SQL query, use the sql() method on the SparkSession instance\n",
    "# All spark.sql queries executed in this manner return a DataFrame on which you may perform \n",
    "# further Spark operations\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".count() # SQL => DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop The Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the underlying SparkContext.\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print(\"Spark session does not context exist - nothing to stop.\")\n",
    "else:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Now you know the concept of Apache Spark, have understood RDDs, worked with dataframes and datasets and used SQL like queries. Let's test the knowledge a do a little exercise.*\n",
    "\n",
    "**Next UP: [Exercise](./05_1_Exercise.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

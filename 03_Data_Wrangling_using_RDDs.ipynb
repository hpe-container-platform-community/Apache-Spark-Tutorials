{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling using RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting/initialising Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programming language Python is used for the implementation in this course - for this we use 'pyspark. (PySpark documentation https://spark.apache.org/docs/latest/api/python/)\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipmort libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Data Analysis\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.master', 'local')\n",
      "('spark.app.startTime', '1643722169725')\n",
      "('spark.app.id', 'local-1643722170645')\n",
      "('spark.driver.port', '55393')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.driver.host', '192.168.178.62')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'Data Analysis')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# check (try) if Spark context variable (sc) exists and print information about the Spark context\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"Spark context does not context exist. Please create Spark context first (run cell above).\")\n",
    "else:\n",
    "    configurations = sc.getConf().getAll()\n",
    "    for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.178.62:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Data Analysis>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print link to Spark UI, Version, Master and AppName\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *For the Tutorials I will be using MovieLens 1M Dataset you can get it from the [Grouplens](https://grouplens.org/datasets/movielens/) website.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets read in the ratings.dat nad create a ratings RDDs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data dataset is pointed to by path.\n",
    "# The path can be either a single text file or a directory - in this case a sinlge file\n",
    "ratingsRDD = sc.textFile(\"data/ml-1m/ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows of the RDD\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thats it We have read the Text file and we are printing out the first 5 rows using `take action` and make sure you don't use a collect action here because that will printout the whole RDD.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now if you check the readme file provided in the Dataset these are the columns in the Data*\n",
    "\n",
    ">*UserID::MovieID::Rating::Timestamp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets check counts on each ratings given, But first we need to split our data and for that we need to make use of a Transformation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each row of the text file at '::' string and select the third element of each row\n",
    "ratings = ratingsRDD.map(lambda x: x.split('::')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the class type of the 'ratings' object\n",
    "# type() method returns class type of the argument(object) passed as parameter.\n",
    "# Knowing the class type is often important in order to be able to use the object correctly in other functions.\n",
    "type(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '3', '3', '4', '5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "result = ratings.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the class type of the 'ratings' object\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'5': 226310, '3': 261197, '4': 348971, '2': 107557, '1': 56174})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the 'result' object. The tuples display the rating and the count (rating:count).\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So you can see how easy it was to get the ratings counter. As it has returned a dictionary lets sort and print the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings   Count\n",
      "\n",
      "★         56174\n",
      "★★        107557\n",
      "★★★       261197\n",
      "★★★★      348971\n",
      "★★★★★     226310\n"
     ]
    }
   ],
   "source": [
    "# the object 'results' is of class type collections\n",
    "# import collections library be able to apply function on this type.\n",
    "import collections\n",
    "\n",
    "# sort the collection by ratings to object 'sortedResults'\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "# Let's create a nice output - print heading\n",
    "print(f\"{'Ratings':10}{'Count'}\\n\")\n",
    "# loop through 'sortedResults' and replace rating digit by number of '★'\n",
    "for key, value in sortedResults.items():\n",
    "    print(f\"{'★'* int(key):{10}}{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets look at another example and check which are the most rated movies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie information is stored in the file \"movies.dat\" and of the following format: 'MovieID::Title::Genres'\n",
    "# define the function 'loadMovieNames' to load the file and extract the movie title from each row \n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"data/ml-1m/movies.dat\", encoding= 'ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('::')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather \n",
    "# than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a \n",
    "# large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using \n",
    "# efficient broadcast algorithms to reduce communication cost. For more information please refer to this link\n",
    "# https://spark.apache.org/docs/3.2.0/rdd-programming-guide.html#broadcast-variables.\n",
    "\n",
    "nameDict = sc.broadcast(loadMovieNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the 'MovieID' from the previously created 'ratingsRDD' dataset\n",
    "movies = ratingsRDD.map(lambda x: (int(x.split(\"::\")[1]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1193, 1), (661, 1), (914, 1), (3408, 1), (2355, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows of the 'movies' dataset\n",
    "movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the number of entries for each MovieID\n",
    "movieCounts = movies.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1193, 1725), (661, 525), (914, 636), (3408, 1315), (2355, 1703)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows (output tuple: MovieID, Sum of entries for each MovieID)\n",
    "movieCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipp the tuple. From (MovieID, Sum of entries for each MovieID) to (Sum of entries for each MovieID, MovieID)\n",
    "flipped = movieCounts.map( lambda x : (x[1], x[0]))\n",
    "# sort the entries by sum of entries\n",
    "sortedMovies = flipped.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1725, 1193), (525, 661), (636, 914), (1315, 3408), (1703, 2355)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows of the 'flipped' dataset\n",
    "flipped.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3428, 2858), (2991, 260), (2990, 1196), (2883, 1210), (2672, 480)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows of the 'sorted' dataset\n",
    "sortedMovies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the MovieID with movie names loaded via the 'nameDict' broadcast variable\n",
    "sortedMoviesWithNames = sortedMovies.map(lambda countMovie : (nameDict.value[countMovie[1]], countMovie[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American Beauty (1999)', 3428),\n",
       " ('Star Wars: Episode IV - A New Hope (1977)', 2991),\n",
       " ('Star Wars: Episode V - The Empire Strikes Back (1980)', 2990),\n",
       " ('Star Wars: Episode VI - Return of the Jedi (1983)', 2883),\n",
       " ('Jurassic Park (1993)', 2672),\n",
       " ('Saving Private Ryan (1998)', 2653),\n",
       " ('Terminator 2: Judgment Day (1991)', 2649),\n",
       " ('Matrix, The (1999)', 2590),\n",
       " ('Back to the Future (1985)', 2583),\n",
       " ('Silence of the Lambs, The (1991)', 2578)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first ten rows\n",
    "sortedMoviesWithNames.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now these are top 10 most rated movies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now lets look at movies with most 5 star ratings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::2355::5::978824291',\n",
       " '1::1287::5::978302039',\n",
       " '1::2804::5::978300719',\n",
       " '1::595::5::978824268']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function 'filter_five_star' to filter the rows with five star ratings only\n",
    "def filter_five_star(line):\n",
    "    splited_line= line.split(\"::\")\n",
    "    if splited_line[2] == '5':\n",
    "        return line\n",
    "        \n",
    "# create new dataset using the 'filter_five_star' function\n",
    "five_start_rattingsRDD= ratingsRDD.filter(lambda x: filter_five_star(x))\n",
    "# display the first five rows\n",
    "five_start_rattingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat the steps of the 'top 10 most rated movies' example from above on the five star ratings RDD\n",
    "# let's extract the 'MovieID' from the previously created 'ratingsRDD' dataset\n",
    "five_start_movies = five_start_rattingsRDD.map(lambda x: (int(x.split(\"::\")[1]), 1))\n",
    "# Sum the number of entries for each MovieID\n",
    "five_start_movieCounts = five_start_movies.reduceByKey(lambda x, y: x + y)\n",
    "# flipp the tuple. From (MovieID, Sum of entries for each MovieID) to (Sum of entries for each MovieID, MovieID)\n",
    "flipped = five_start_movieCounts.map( lambda x : (x[1], x[0]))\n",
    "# sort the entries by sum of entries\n",
    "five_start_sortedMovies = flipped.sortByKey(ascending=False)\n",
    "# replace the MovieID with movie names loaded via the 'nameDict' broadcast variable\n",
    "five_start_sortedMoviesWithNames = five_start_sortedMovies.map(lambda countMovie : (nameDict.value[countMovie[1]], countMovie[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('American Beauty (1999)', 1963),\n",
       " ('Star Wars: Episode IV - A New Hope (1977)', 1826),\n",
       " ('Raiders of the Lost Ark (1981)', 1500),\n",
       " ('Star Wars: Episode V - The Empire Strikes Back (1980)', 1483),\n",
       " (\"Schindler's List (1993)\", 1475),\n",
       " ('Godfather, The (1972)', 1475),\n",
       " ('Shawshank Redemption, The (1994)', 1457),\n",
       " ('Matrix, The (1999)', 1430),\n",
       " ('Saving Private Ryan (1998)', 1405),\n",
       " ('Sixth Sense, The (1999)', 1385)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first ten rows\n",
    "five_start_sortedMoviesWithNames.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets look at number of movies produced in each year*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie information in the file \"movies.dat\" is stored in the following format: 'MovieID::Title::Genres'\n",
    "# read the text file and create a new RDD\n",
    "moviesRDD =sc.textFile(\"data/ml-1m/movies.dat\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1::Toy Story (1995)::Animation|Children's|Comedy\",\n",
       " \"2::Jumanji (1995)::Adventure|Children's|Fantasy\",\n",
       " '3::Grumpier Old Men (1995)::Comedy|Romance',\n",
       " '4::Waiting to Exhale (1995)::Comedy|Drama',\n",
       " '5::Father of the Bride Part II (1995)::Comedy']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1995)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is necessary to extract the 'year' of the movie from a string\n",
    "# this could be achieved by using 'array slicing' (part of python language)\n",
    "'Toy Story (1995)'[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1995'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be more flexible in the extraction of the 'year' value we are using regular expressions\n",
    "# therefore we have to import the python library 're'\n",
    "import re\n",
    "# now apply a 'regex' (regular expression) to a string and 'deselect' the parentheses\n",
    "re.search(r'\\([0-9]{4}\\)$','Grumpier Old Men (1995)').group(0)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract the year from every row using regex \n",
    "def get_year(line):\n",
    "    split_line= line.split('::')\n",
    "    year= re.search(r'\\([0-9]{4}\\)$',split_line[1]).group(0)[1:-1]\n",
    "    return (year, 1)\n",
    "\n",
    "# create a dataset with only the year value from each row    \n",
    "year_RDD= moviesRDD.map(lambda x: get_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1995', 1), ('1995', 1), ('1995', 1), ('1995', 1), ('1995', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "year_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the number of entries for each year\n",
    "yearCounts = year_RDD.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1995', 342), ('1994', 257), ('1996', 345), ('1976', 21), ('1993', 165)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "yearCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the entries ascending by the 'year'\n",
    "ascending_sorted_yearCounts = yearCounts.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1919', 3), ('1920', 2), ('1921', 1), ('1922', 2), ('1923', 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "ascending_sorted_yearCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the entries descending by the 'year'\n",
    "descending_sorted_yearCounts = yearCounts.sortByKey(ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000', 156), ('1999', 283), ('1998', 337), ('1997', 315), ('1996', 345)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "descending_sorted_yearCounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Years with most movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipp the tuple. From (year, Sum of entries for each year) to (Sum of entries for each year, year)\n",
    "flipped = yearCounts.map( lambda x : (x[1], x[0]))\n",
    "# sort the entries by sum of entries for each year\n",
    "descending_sorted_yearCounts = flipped.sortByKey(ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(345, '1996'),\n",
       " (342, '1995'),\n",
       " (337, '1998'),\n",
       " (315, '1997'),\n",
       " (283, '1999'),\n",
       " (257, '1994'),\n",
       " (165, '1993'),\n",
       " (156, '2000'),\n",
       " (104, '1986'),\n",
       " (102, '1992')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first ten rows\n",
    "descending_sorted_yearCounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets find out the which age group is most active on the platform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows of the original ratingsRDD\n",
    "# date format of the data is UserID::MovieID::Rating::Timestamp\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User information is stored in the file \"users.dat\" and of the following \n",
    "# format: 'UserID::Gender::Age::Occupation::Zip-code'\n",
    "# The values in the Age field only have values from the set 1, 18, 25, 35, 45, 50, 56. \n",
    "# This corresponds to the age groups (see readme file).\n",
    "\n",
    "# Define the function 'load_age_group' to load the file and extract the age/age group from each row\n",
    "def load_age_group():\n",
    "    # define dictionary 'age_group' with 7 bins\n",
    "    age_group= {'1':  \"Under 18\", '18':  \"18-24\", '25':  \"25-34\", '35':  \"35-44\", '45':  \"45-49\", '50':  \"50-55\", '56':  \"56+\"}\n",
    "    # define empty dictionary 'user_ageGroup'\n",
    "    user_ageGroup = {}\n",
    "    # open the file and 'loop' trough all lines\n",
    "    with open(\"data/ml-1m/users.dat\") as f:\n",
    "        for line in f:\n",
    "            # select age fieled value\n",
    "            fields = line.split('::')\n",
    "            # add info to the dict user_ageGroup) and replace the age value by the \n",
    "            # corresponding age_group range value: 18 -> \"18-24\"\n",
    "            user_ageGroup[int(fields[0])] = age_group[fields[2]]\n",
    "    return user_ageGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define broadcast variable ageGroupDict and load age_groups from file (using function load_age_group)\n",
    "ageGroupDict = sc.broadcast(load_age_group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Under 18'), (2, '56+'), (3, '25-34'), (4, '45-49'), (5, '25-34')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first 5 istems of the 'ageGroupDict' broadcast variable\n",
    "list(ageGroupDict.value.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the UserID field from ratings dataset\n",
    "users_ratings = ratingsRDD.map(lambda x: (int(x.split(\"::\")[0]), 1))\n",
    "# sum the number of ratings by UserID\n",
    "count_user_ratings = users_ratings.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 53), (2, 129), (3, 51), (4, 21), (5, 198)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "count_user_ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipp the tuple. From (UserID, Sum of entries for each UserID) to (Sum of entries for each UserID, UserID)\n",
    "flipped = count_user_ratings.map( lambda x : (x[1], x[0]))\n",
    "# replace the UserID with the age group of the user\n",
    "age_group_count = flipped.map(lambda countuser : (ageGroupDict.value[countuser[1]], countuser[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the number of ratings by age group\n",
    "age_group_counts= age_group_count.reduceByKey(lambda x , y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('25-34', 395556),\n",
       " ('35-44', 199003),\n",
       " ('18-24', 183536),\n",
       " ('45-49', 83633),\n",
       " ('50-55', 72490),\n",
       " ('56+', 38780),\n",
       " ('Under 18', 27211)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the age_age_group_counts dataset ascending on age groups\n",
    "age_group_counts.map(lambda x: (x[1], x[0])).sortByKey(ascending= False).map(lambda x: (x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets Load in another fake social network dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new RDD friends and load a text file \n",
    "# data format structure is ID, Name, Age, Number of fake friends\n",
    "friends = sc.textFile(\"data/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "friends.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the number of rows in the dataset\n",
    "friends.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets look at the average number of friends broken down by age in this Dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the funtion parseLine to extract age and numFriends fields from one row\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 385), (26, 2), (55, 221), (40, 465), (68, 21)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the friendsRDD by extracting age and numFriends fields from all rows (using the parseLine function)\n",
    "friendsRDD = friends.map(parseLine)\n",
    "# display the first five rows\n",
    "friendsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each age we want to sum the number of fake friends AND how many row with the age exist.\n",
    "# Therefore we add a 'tuple' to the 'number of fake friends' field with the second value as '1'.\n",
    "# The output of the 'mapValues(lambda x: (x, 1))' function would look like:\n",
    "# [(33, (385, 1)), (26, (2, 1)), (55, (221, 1)), (40, (465, 1)), (68, (21, 1))]\n",
    "\n",
    "# In the second part of the function ('reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))')\n",
    "# the sum of the 'number of fake friends' and the '1' values for each 'age' is formed.\n",
    "totalsByAge = friendsRDD.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(33, (3904, 12)),\n",
       " (26, (4115, 17)),\n",
       " (55, (3842, 13)),\n",
       " (40, (4264, 17)),\n",
       " (68, (2696, 10))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "totalsByAge.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate the average of fake friends for a person of a specific age we need to divide \n",
    "# the 'sum of fake friends' by 'number of people with specific age'\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: round(x[0] / x[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 325.33), (26, 242.06), (55, 295.54), (40, 250.82), (68, 269.6)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "averagesByAge.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets load up another dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new RDD friends and load a text file \n",
    "# data format structure is stationID, timestamp, entryType, temperature ... followed by none relevant fields\n",
    "temp = sc.textFile(\"data/1800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITE00100554,18000101,TMAX,-75,,,E,',\n",
       " 'ITE00100554,18000101,TMIN,-148,,,E,',\n",
       " 'GM000010962,18000101,PRCP,0,,,E,',\n",
       " 'EZE00100082,18000101,TMAX,-86,,,E,',\n",
       " 'EZE00100082,18000101,TMIN,-135,,,E,']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "temp.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets check the weather stations with minimum temperatures in 1800.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the funtion parseLine to extract stationID, entryType and numFriends fields from one row\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new RDD with fields stationID, entryType and temperature\n",
    "tempRDD = temp.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMAX', 18.5),\n",
       " ('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('GM000010962', 'PRCP', 32.0),\n",
       " ('EZE00100082', 'TMAX', 16.52),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "tempRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows with entryType 'TMIN'\n",
    "minTemps = tempRDD.filter(lambda x: \"TMIN\" in x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999),\n",
       " ('ITE00100554', 'TMIN', 9.5),\n",
       " ('EZE00100082', 'TMIN', 8.599999999999998),\n",
       " ('ITE00100554', 'TMIN', 23.72)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "minTemps.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select field \"0\" (stationID) and field \"2\" (temperature) and create dataset stationTemps\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.359999999999999),\n",
       " ('EZE00100082', 7.699999999999999),\n",
       " ('ITE00100554', 9.5),\n",
       " ('EZE00100082', 8.599999999999998),\n",
       " ('ITE00100554', 23.72)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "stationTemps.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the minimum (rounded) of each stationID\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: round(min(x,y), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.36), ('EZE00100082', 7.7)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the dataset - only two station reported minimum temperatures\n",
    "minTemps.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets do another word count on a text file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset book and read the text file\n",
    "book = sc.textFile(\"data/Book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment: Building an Internet Business of One',\n",
       " 'Achieving Financial and Personal Freedom through a Lifestyle Technology Business']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first two rows\n",
    "book.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function normalizeWords.\n",
    "\n",
    "# The function uses re.compile(). Python’s re.compile() method is used to compile a regular expression \n",
    "# pattern provided as a string into a regex pattern object (re.Pattern). The command re.compile is explained \n",
    "# in the Python docs (https://docs.python.org/3/library/re.html#re.compile)\n",
    "\n",
    "# The specific regex expression searches for groups that have alphanumerics (that's the \\w part) or \n",
    "# apostrophes (which is also in those square brackets) that are 1 or longer. \n",
    "# Note that whitespace is not a match, so this, generally speaking, breaks a line into words.\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset word by using the function normalizeWords and the Spark flatmap transformation\n",
    "\n",
    "# flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) \n",
    "# after applying the function on every element and returns a new PySpark RDD/DataFrame.\n",
    "\n",
    "words = book.flatMap(normalizeWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of occurrences of each word in the text\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('self', 111),\n",
       " ('employment', 75),\n",
       " ('building', 33),\n",
       " ('an', 178),\n",
       " ('internet', 26)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first five rows\n",
    "wordCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the wordsCount dataset descending by he number of occurrences\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1878, 'you'),\n",
       " (1828, 'to'),\n",
       " (1420, 'your'),\n",
       " (1292, 'the'),\n",
       " (1191, 'a'),\n",
       " (970, 'of'),\n",
       " (934, 'and'),\n",
       " (772, ''),\n",
       " (747, 'that'),\n",
       " (649, 'it')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first ten rows\n",
    "wordCountsSorted.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the underlying SparkContext.\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"Spark context does not context exist - nothing to stop.\")\n",
    "else:\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*This Notebook was an introduction to how to work with data using RDDs (Resilient distributed\n",
    "datasets). Now we want to introduce a more modern way using Apache Spark using Structured APIs.*\n",
    "\n",
    "**Next UP: [Structured APIs](./04_Structured_APIs.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
